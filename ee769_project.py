# -*- coding: utf-8 -*-
"""EE769_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5Ii258X0oE1V0dh4Il1zFQ3oALHjOVa

---
# EE769 Course Project
#####Title: Plant Disease Prediction using Machine Learning
#####Members: 
<ul>
<li> Gazula Srinivas Bapaiah Naidu (203079007)</li>
<li> Madhav Prabhakar Nadkarni (213070065)</li>
<li> Pranav Patel (18D070057)</li>
</ul>

####Problem statement:
* To develop a model that can detect the plant disease with precision and to develop an user application  which makes use of the developed model.

#####User Application link: https://www.ee.iitb.ac.in/course/~gsbnaidu/

####Project Type
* Replicating an advanced research paper
* Analysis or comparison of established techniques using experiments
* Deployment and application development around an ML model or pipeline

####Data sets to be used 
* PlantVillage Dataset (https://github.com/spMohanty/PlantVillage-Dataset)

####ML Techniques to be used
* Random forest classifier
* L2 regularized logistic regression
* Decision Tree
* Adaboost Decision Tree
* Voting Classifier(Ensemble Learning Model of the above three)
* Deep learning model

####GitHub Repo
* https://github.com/srinivasgazula/EE769_Course_Project

####Project Report
* https://www.ee.iitb.ac.in/course/~gsbnaidu/assets/files/EE769_Report.pdf
---

* Modules that are used in the assignment are declared in below code block
"""

#Mentiong all the modules that are used

# !pip3 install keras
# !pip3 install tensorflow

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
# Standardize the data
from sklearn.preprocessing import StandardScaler
# Modeling 
from sklearn.model_selection import train_test_split
# Hyperparameter tuning
from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score, roc_curve, cohen_kappa_score, matthews_corrcoef, log_loss, make_scorer, recall_score, precision_score
from sklearn.metrics import SCORERS
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
import six
import sys
sys.modules['sklearn.externals.six'] = six
# from io import StringIO
from sklearn import tree
import graphviz
# from IPython.display import Image 
from pydot import graph_from_dot_data
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import AdaBoostClassifier

import pickle

import torchvision.models as models
import torchvision.transforms as transforms
import torch
from torch.autograd import Variable
from PIL import Image
from torchvision.models import ResNet18_Weights
import warnings
import time
import os
import copy

from google.colab import drive
drive.mount('/content/drive')
# drive.mount('/content/drive', force_remount=True)

import warnings

import tensorflow as tf
from tensorflow.keras.layers import Input, Flatten, Dense, concatenate, Dropout,Concatenate
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.optimizers import Adam

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from keras.utils.vis_utils import plot_model
import itertools

from tensorflow.keras.preprocessing.image import load_img, img_to_array

"""## Task-1A (Load Training Data into csv)
* Write a function that outputs ResNet18 features for a given input image. Extract features for training images 
* You should get an Nx512 dimensional array
"""

#References:
#1. https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c
#2. https://stackoverflow.com/questions/61606416/runtimeerror-output-with-shape-512-doesnt-match-the-broadcast-shape-1-512


#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')

#Set model to evaluation mode
model.eval()

#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()

def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)

  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))

  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)

  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)

  #Run the model on our transformed image
  model(t_img)

  # Detach our copy function from the layer
  h.remove()

  # Return the feature vector 
  return features_of_image.numpy()


path =  '/content/drive/MyDrive/EE769/project_dataset/train'

plants_list = []  #['Orange', 'Peach', 'Squash', 'Strawberry', 'Soyabean', 'Corn_maize', 'Apple', 'Grape', 'Potato', 'Pepper', 'Raspberry', 'Blueberry', 'Tomato', 'Cherry']
diseases_dict = {}
obj = os.scandir(path)
for entry in obj:
    if entry.is_dir():
        plants_list.append(entry.name)

print(plants_list)

features_array = []
target_plants_array = []
target_diseases_array = []

for x in plants_list:
  plants_path = path + str('/') + x
  temp_list = []
  obj = os.scandir(plants_path)
  for entry in obj:
    if entry.is_dir():
      temp_list.append(entry.name)
  diseases_dict[x] = temp_list
  for y in temp_list:
    disease_path = plants_path+ str('/') + y
    obj = os.scandir(disease_path)
    # count = 0
    for entry in obj:
      # if count<2:
      if entry.is_file():
        file_path = disease_path + str('/') + entry.name
        features_array.append(extract_ResNet18_features(file_path))
        target_plants_array.append(x)
        target_diseases_array.append(y)
      #     count += 1
      # else:
      #   break

rows = len(features_array)
cols = len(features_array[0])

print('-------------------------------------------------------------------\n')
print('Dimensions of features array list is ' + str(rows) + ' X ' + str(cols) + '\n')
print('-------------------------------------------------------------------\n')

#Converting feature data, target data into dataframes
features_list = []
for x in range(cols):
  features_list.append(str(x))
X = pd.DataFrame(features_array, columns = features_list)
y1_train = pd.DataFrame({'plant':target_plants_array})
y2_train = pd.DataFrame({'disease':target_diseases_array})

y1_classes = np.unique(y1_train) #plants
y2_classes = np.unique(y2_train) #diseases

print(y1_train.value_counts().reset_index())
print(y2_train.value_counts().reset_index())

# # # Multivariate feature imputation
# imp = IterativeImputer(max_iter=10, random_state=10)
# imp = imp.fit(X)
# X1_train = imp.transform(X)
# X1_train = pd.DataFrame(X1_train, columns = X.columns)

X1_train = X
X2_train = X1_train

df = pd.concat([X1_train, y1_train, y2_train], axis=1)
df.to_csv('/content/drive/MyDrive/EE769/data_tain.csv')

print('The obtained features data array is:')
display(pd.concat([X1_train, y1_train, y2_train], axis=1))
print('-------------------------------------------------------------------\n')

"""## Task-1B (Load Validation Data into csv)
* Write a function that outputs ResNet18 features for a given input image. Extract features for validation images 
* You should get an Nx512 dimensional array
"""

#References:
#1. https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c
#2. https://stackoverflow.com/questions/61606416/runtimeerror-output-with-shape-512-doesnt-match-the-broadcast-shape-1-512


#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')

#Set model to evaluation mode
model.eval()

#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()

def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)

  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))

  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)

  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)

  #Run the model on our transformed image
  model(t_img)

  # Detach our copy function from the layer
  h.remove()

  # Return the feature vector
  return features_of_image.numpy()


path =  '/content/drive/MyDrive/EE769/project_dataset/valid'

plants_list = []  #['Orange', 'Peach', 'Squash', 'Strawberry', 'Soyabean', 'Corn_maize', 'Apple', 'Grape', 'Potato', 'Pepper', 'Raspberry', 'Blueberry', 'Tomato', 'Cherry']
diseases_dict = {}
obj = os.scandir(path)
for entry in obj:
    if entry.is_dir():
        plants_list.append(entry.name)

print(plants_list)

features_array = []
target_plants_array = []
target_diseases_array = []

for x in plants_list:
  plants_path = path + str('/') + x
  temp_list = []
  obj = os.scandir(plants_path)
  for entry in obj:
    if entry.is_dir():
      temp_list.append(entry.name)
  diseases_dict[x] = temp_list
  for y in temp_list:
    disease_path = plants_path+ str('/') + y
    obj = os.scandir(disease_path)
    # count = 0
    for entry in obj:
      # if count<10:
      if entry.is_file():
         file_path = disease_path + str('/') + entry.name
         features_array.append(extract_ResNet18_features(file_path))
         target_plants_array.append(x)
         target_diseases_array.append(y)
      #   count += 1
      # else:
      #   break

rows = len(features_array)
cols = len(features_array[0])

print('-------------------------------------------------------------------\n')
print('Dimensions of features array list is ' + str(rows) + ' X ' + str(cols) + '\n')
print('-------------------------------------------------------------------\n')

#Converting feature data, target data into dataframes
features_list = []
for x in range(cols):
  features_list.append(str(x))
X = pd.DataFrame(features_array, columns = features_list)
y1_valid = pd.DataFrame({'plant':target_plants_array})
y2_valid = pd.DataFrame({'disease':target_diseases_array})

y1_valid_classes = np.unique(y1_valid) #plants
y2_valid_classes = np.unique(y2_valid) #diseases

print(y1_valid.value_counts().reset_index())
print(y2_valid.value_counts().reset_index())

# # Multivariate feature imputation
# imp = IterativeImputer(max_iter=10, random_state=10)
# imp = imp.fit(X)
# X1_valid = imp.transform(X)
# X1_valid = pd.DataFrame(X1_valid, columns = X.columns)

X1_valid = X
X2_valid = X1_valid

df = pd.concat([X1_valid, y1_valid, y2_valid], axis=1)
df.to_csv('/content/drive/MyDrive/EE769/data_valid.csv')

print('The obtained features data array is:')
display(pd.concat([X1_valid, y1_valid, y2_valid], axis=1))
print('-------------------------------------------------------------------\n')

"""## Task-1C (Load Train and valid data from csv)"""

def initialize_train_valid_data():
  data_train = pd.read_csv('/content/drive/MyDrive/EE769/data_tain.csv')

  # data_train = pd.read_csv('/content/drive/MyDrive/EE769/data_test.csv')

  features_list = list( data_train.loc[:][:1] )[1:-2]
  target_list  = list( data_train.loc[:][:1] )[-2:]

  #Extracting feature data from csv
  X1_train  = data_train.loc[:,features_list]
  X2_train = X1_train

  #Extracting plants target data from csv
  y1_train = data_train.loc[:,target_list[0]]
  #Extracting diseases target data from csv
  y2_train = data_train.loc[:,target_list[1]]

  y1_classes = np.unique(y1_train) #plants
  y2_classes = np.unique(y2_train) #diseases


  # ------------------------------------------
  data_valid = pd.read_csv('/content/drive/MyDrive/EE769/data_valid.csv')

  features_list = list( data_valid.loc[:][:1] )[1:-2]
  target_list  = list( data_valid.loc[:][:1] )[-2:]

  #Extracting feature data from csv
  X1_valid  = data_valid.loc[:,features_list]
  X2_valid = X1_valid

  #Extracting plants target data from csv
  y1_valid = data_valid.loc[:,target_list[0]]
  #Extracting diseases target data from csv
  y2_valid = data_valid.loc[:,target_list[1]]

  y1_valid_classes = np.unique(y1_valid) #plants
  y2_valid_classes = np.unique(y2_valid) #diseases
  return X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes
###--END OF FUNCTION--############
  
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()

print(y1_train.value_counts().reset_index())
print(y2_train.value_counts().reset_index())
print('-------------------------------------------------------------------\n')
display(pd.concat([X1_train, y1_train, y2_train], axis=1))
print('-------------------------------------------------------------------\n')
print(y1_valid.value_counts().reset_index())
print(y2_valid.value_counts().reset_index())
print('-------------------------------------------------------------------\n')
display(pd.concat([X1_valid, y1_valid, y2_valid], axis=1))

"""## Task-1D (Are the classes balanced?)"""

#References:
#1. https://stackoverflow.com/questions/67287472/how-can-i-find-whether-my-dataset-is-balanced-or-not#:~:text=In%20simple%20words%2C%20you%20need,present%20in%20your%20target%20variable.&text=If%20you%20check%20the%20ratio,oversample%20or%20undersample%20the%20data
#2. https://www.tutorialspoint.com/how-to-plot-two-seaborn-lmplots-side-by-side-matplotlib

print("Training Dataset")
plt.rcParams["figure.figsize"] = [5.00, 3.50]
plt.rcParams["figure.autolayout"] = True
# f, axes = plt.subplots(2, 1)
xx1 = y1_train.value_counts().reset_index()
sns.barplot( y='index', x='plant',data=xx1, palette='flare').set(xlabel=None, ylabel=None)
plt.savefig('/content/drive/MyDrive/EE769/Distribution_Plants.png')
plt.show()

plt.rcParams["figure.figsize"] = [5.00, 3.50]
plt.rcParams["figure.autolayout"] = True
xx2 = y2_train.value_counts().reset_index()
sns.barplot( y='index', x='disease',data=xx2, palette='cividis').set(xlabel=None, ylabel=None)
plt.savefig('/content/drive/MyDrive/EE769/Distribution_Diseases.png')
plt.show()

print("Validation Dataset")
plt.rcParams["figure.figsize"] = [5.00, 3.50]
plt.rcParams["figure.autolayout"] = True
# f, axes = plt.subplots(2, 1)
xx1 = y1_valid.value_counts().reset_index()
sns.barplot( y='index', x='plant',data=xx1, palette='flare').set(xlabel=None, ylabel=None)
plt.savefig('/content/drive/MyDrive/EE769/Valid_Distribution_Plants.png')
plt.show()

plt.rcParams["figure.figsize"] = [5.00, 3.50]
plt.rcParams["figure.autolayout"] = True
xx2 = y2_valid.value_counts().reset_index()
sns.barplot( y='index', x='disease',data=xx2, palette='cividis').set(xlabel=None, ylabel=None)
plt.savefig('/content/drive/MyDrive/EE769/Valid_Distribution_Diseases.png')
plt.show()

"""## Task-2A: Predictor-1 
### L2 regularized logistic regression - Training
"""

#L2 regularized logistic regression

#References:
#1. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
#2. https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a

clf_plants = LogisticRegression(random_state=0,max_iter=5000)
clf_plants.fit(X1_train, y1_train.values.ravel())


clf_diseases = LogisticRegression(random_state=0,max_iter=10000)
clf_diseases.fit(X2_train, y2_train.values.ravel())

#For Deployment:
with open('/content/drive/MyDrive/EE769/L2_regularized_plants_model.bin','wb') as f_out1: #specify the file where you wnt to save
  pickle.dump(clf_plants,f_out1) #Save the model to file with pickle

with open('/content/drive/MyDrive/EE769/L2_regularized_diseases_model.bin','wb') as f_out2: #specify the file where you wnt to save
  pickle.dump(clf_diseases,f_out2) #Save the model to file with pickle

"""## Task-2B: Predictor-1 
###Validation - L2 regularized logistic regression
"""

#function to plot AUC ROC Curve for multi Classification
def plot_auc_roc_multi(y_valid, y_classes, pred_prob2,model_label):
  colors = ['orange','green','blue','red','purple','brown','pink','gray','olive','cyan','salmon','tan','yellow','greenyellow','aqua','beige','lime','dodgerblue','magenta','tomato','teal','silver']
  fpr = {}
  tpr = {}
  thresh ={}
  n_class = len(y_classes)
  for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_valid, pred_prob2[:,i], pos_label=y_classes[i])
    # plotting 
    plt.plot(fpr[i], tpr[i], linestyle='--',color=colors[i], label=y_classes[i] + ' vs Rest')

  plt.title(model_label)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  plt.show()
  return
#####END of function#############

with open('/content/drive/MyDrive/EE769/L2_regularized_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/L2_regularized_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_valid_predicted = clf_plants_loaded.predict(X1_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_valid, y1_valid_predicted), columns=y1_valid_classes, index=y1_valid_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/L2_Plants.png')

print('Plants Classifaction:')

print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y1_valid, y1_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y1_valid, y1_valid_predicted):.4f}')
pred_prob1 = clf_plants_loaded.predict_proba(X1_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y1_valid, pred_prob1,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y1_valid, y1_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_valid_predicted = clf_diseases_loaded.predict(X2_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_valid, y2_valid_predicted), columns=y2_classes, index=y2_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/L2_Diseases.png')
print('Diseases Classifaction:')

print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y2_valid, y2_valid_predicted):.4f}')
pred_prob2 = clf_diseases_loaded.predict_proba(X2_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y2_valid, pred_prob2,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y2_valid, y2_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

plot_auc_roc_multi(y1_valid, y1_classes, pred_prob1,'Plants')
print('-------------------------------------------------------------------\n')
plot_auc_roc_multi(y2_valid, y2_classes, pred_prob2,'Diseases')

"""## Task-2C: Predictor-1 
###Deployment trail run - L2 regularized logistic regression

1. https://freecontent.manning.com/deploying-machine-learning-models-part-1-saving-models/
"""

with open('/content/drive/MyDrive/EE769/L2_regularized_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/L2_regularized_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

# print(clf_plants_loaded.get_params())
# print(clf_diseases_loaded.get_params())

#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')
#Set model to evaluation mode
model.eval()
#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()
def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)
  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)
  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)
  #Run the model on our transformed image
  model(t_img)
  # Detach our copy function from the layer
  h.remove()
  # Return the feature vector
  return features_of_image

features_list = []
for x in range(512):
  features_list.append(str(x))
  
#input image
input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Apple/Apple_scab/AppleScab2.JPG'
input_data_array = pd.DataFrame([extract_ResNet18_features(input_image).numpy()],columns=features_list)
display(input_data_array)
print('The Predicted plant is '+ str(clf_plants_loaded.predict(input_data_array)[0]) )
print('The Predicted disease is '+ str(clf_diseases_loaded.predict(input_data_array)[0]) )

"""## Task-3A: Predictor-2 (Plants Classification)
### Random Forest -  Random Search CV - Training
"""

#Random Forest

#References:
#1. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
#2. https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/
#3. https://towardsdatascience.com/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399


# max_depth_range = np.logspace(0,1,10).astype(int)
max_depth_range = [40]
print(f'The list of values for max_depth are {max_depth_range}')
print('-------------------------------------------------------------------\n')
#Hyperparameter Tuning Using Grid Search
param_grid = { 
    "max_depth": max_depth_range,
    "max_features" : ['sqrt',None]
    # 'max_features' : hp.choice('max_features', ['sqrt','log2',None])
    }

# Set up score
# scoring = ['accuracy']
scoring = {
    'accuracy_score': make_scorer(accuracy_score),
    'balanced_accuracy_score': make_scorer(balanced_accuracy_score),
    # 'recall_score': make_scorer(recall_score)
    # 'precision_score': make_scorer(precision_score)
    # 'roc_auc_score': make_scorer(roc_auc_score)
    'cohen_kappa_score': make_scorer(cohen_kappa_score)
}
# Set up the k-fold cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

#Plants Classifaction

clf_plants = RandomForestClassifier(random_state=44)
# Define grid search
grid_search = RandomizedSearchCV(estimator=clf_plants, 
                           param_distributions=param_grid, 
                           scoring=scoring, 
                           refit='accuracy_score', 
                           n_jobs=-1, 
                           cv=kfold, 
                           verbose=True)
# Fit grid search
grid_result = grid_search.fit(X1_train, y1_train.values.ravel())

#For Deployment:
clf_plants = RandomForestClassifier(random_state=44,max_depth=grid_result.best_params_['max_depth'],max_features=grid_result.best_params_['max_features'])
clf_plants.fit(X1_train, y1_train.values.ravel())
with open('/content/drive/MyDrive/EE769/Random_forest_plants_model_Random.bin','wb') as f_out1: #specify the file where you wnt to save
  pickle.dump(clf_plants,f_out1) #Save the model to file with pickle

"""## Task-3A: Predictor-2 (Diseases Classification) - Part-1
### Random Forest -  Random Search CV - Training
"""

# warnings.filterwarnings('ignore')
#Random Forest

#References:
#1. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
#2. https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/
#3. https://towardsdatascience.com/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399


# max_depth_range = np.logspace(0,1,10).astype(int)
max_depth_range = [40]
print(f'The list of values for max_depth are {max_depth_range}')
print('-------------------------------------------------------------------\n')
#Hyperparameter Tuning Using Grid Search
param_grid = { 
    "max_depth": max_depth_range,
    "max_features" : ['sqrt']
    # 'max_features' : hp.choice('max_features', ['sqrt','log2',None])
    }

# Set up score

# Set up the k-fold cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

#Diseases Classifaction
clf_diseases = RandomForestClassifier(random_state=44)
# Define grid search
grid_search = RandomizedSearchCV(estimator=clf_diseases,
                                 param_distributions=param_grid,
                                 scoring='accuracy',
                                 n_jobs=-1,
                                 cv=kfold,
                                 verbose=True)
# Fit grid search
grid_result = grid_search.fit(X2_train, y2_train.values.ravel())

#For Deployment:
clf_diseases = RandomForestClassifier(random_state=44,max_depth=grid_result.best_params_['max_depth'],max_features=grid_result.best_params_['max_features'])
clf_diseases.fit(X2_train, y2_train.values.ravel())
with open('/content/drive/MyDrive/EE769/Random_forest_diseases_model_Random.bin','wb') as f_out2: #specify the file where you wnt to save
  pickle.dump(clf_diseases,f_out2) #Save the model to file with pickle

"""## Task-3B Predictor-2
### Validation - Random Forest
"""

#function to plot AUC ROC Curve for multi Classification
def plot_auc_roc_multi(y_valid, y_classes, pred_prob2,model_label):
  colors = ['orange','green','blue','red','purple','brown','pink','gray','olive','cyan','salmon','tan','yellow','greenyellow','aqua','beige','lime','dodgerblue','magenta','tomato','teal','silver']
  fpr = {}
  tpr = {}
  thresh ={}
  n_class = len(y_classes)
  for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_valid, pred_prob2[:,i], pos_label=y_classes[i])
    # plotting 
    plt.plot(fpr[i], tpr[i], linestyle='--',color=colors[i], label=y_classes[i] + ' vs Rest')

  plt.title(model_label)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  plt.show()
  return
#####END of function#############

with open('/content/drive/MyDrive/EE769/Random_forest_plants_model_Random.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Random_forest_diseases_model_Random.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_valid_predicted = clf_plants_loaded.predict(X1_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_valid, y1_valid_predicted), columns=y1_valid_classes, index=y1_valid_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Random_Plants_Random.png')

# grid_result
print('Plants Classifaction:')
# Print the hyperparameters for the best score
print(f'\tThe hyperparameters are {clf_plants_loaded.get_params()}')
# print(f'\tThe best hyperparameters are {grid_result.best_params_}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y1_valid, y1_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y1_valid, y1_valid_predicted):.4f}')
pred_prob1 = clf_plants_loaded.predict_proba(X1_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y1_valid, pred_prob1,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y1_valid, y1_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_valid_predicted = clf_diseases_loaded.predict(X2_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_valid, y2_valid_predicted), columns=y2_valid_classes, index=y2_valid_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Random_diseases_Random.png')

# grid_result
print('Diseases Classifaction:')
# Print the hyperparameters for the best score
print(f'\tThe hyperparameters are {clf_diseases_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y2_valid, y2_valid_predicted):.4f}')
pred_prob2 = clf_diseases_loaded.predict_proba(X2_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y1_valid, pred_prob1,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y2_valid, y2_valid_predicted))
plt.show()

print('-------------------------------------------------------------------\n')
plot_auc_roc_multi(y1_valid, y1_classes, pred_prob1,'Plants')
print('-------------------------------------------------------------------\n')
plot_auc_roc_multi(y2_valid, y2_classes, pred_prob2,'Diseases')

"""## Task-3C: Predictor-2 
###Deployment trail run -  - Random Forest

1. https://freecontent.manning.com/deploying-machine-learning-models-part-1-saving-models/
"""

with open('/content/drive/MyDrive/EE769/Random_forest_plants_model_Random.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Random_forest_diseases_model_Random.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

# print(clf_plants_loaded.get_params())
# print(clf_diseases_loaded.get_params())

#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')
#Set model to evaluation mode
model.eval()
#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()
def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)
  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)
  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)
  #Run the model on our transformed image
  model(t_img)
  # Detach our copy function from the layer
  h.remove()
  # Return the feature vector
  return features_of_image

features_list = []
for x in range(512):
  features_list.append(str(x))

#input image
input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Apple/Apple_scab/AppleScab1.JPG'
input_data_array = pd.DataFrame([extract_ResNet18_features(input_image).numpy()],columns=features_list)
display(input_data_array)
print('The Predicted plant is '+ str(clf_plants_loaded.predict(input_data_array)[0]) )
print('The Predicted disease is '+ str(clf_diseases_loaded.predict(input_data_array)[0]) )

"""## Task-4A: Predictor-3A (Plants Prediction)
### Decision Tree - Random Search CV - Training
"""

#Decision Tree

#References:
#1. https://towardsdatascience.com/decision-tree-in-python-b433ae57fb93
#2. https://mathtuition88.com/2019/10/11/how-to-save-sklearn-tree-plot-as-file-vector-graphics/

max_depth_range = [18,19,20]
min_samples_split_range = np.logspace(-5, -4, 3, endpoint=True)
min_impurity_decrease_range = np.logspace(-5, -4, 3, endpoint=True)
min_samples_leaf_range = np.logspace(-9, -7, 3, endpoint=True)
max_leaf_nodes_range = [1000,1200,1500]
# min_samples_split_range = np.linspace(0.0001, 0.001, 3, endpoint=True)
print(f'The list of values for max_depth are {max_depth_range}')
print(f'The list of values for min_samples_split are {min_samples_split_range}')
print(f'The list of values for min_impurity_decrease are {min_impurity_decrease_range}')
print(f'The list of values for min_samples_leaf are {min_samples_leaf_range}')
print(f'The list of values for max_leaf_nodes are {max_leaf_nodes_range}')
print('-------------------------------------------------------------------\n')
#Hyperparameter Tuning Using Grid Search
param_grid = { 
    "max_depth": max_depth_range,
    "max_features" : ['sqrt','log2',None],
    "criterion" : ['gini','entropy','log_loss'],
    "min_samples_split" : min_samples_split_range,
    "min_impurity_decrease" : min_impurity_decrease_range,
    "min_samples_leaf" : min_samples_leaf_range,
    "max_leaf_nodes" : max_leaf_nodes_range
    }

# Set up score
scoring = {
    'accuracy_score': make_scorer(accuracy_score),
    'balanced_accuracy_score': make_scorer(balanced_accuracy_score),
    # 'recall_score': make_scorer(recall_score)
    # 'precision_score': make_scorer(precision_score)
    # 'roc_auc_score': make_scorer(roc_auc_score)
    'cohen_kappa_score': make_scorer(cohen_kappa_score)
}
# Set up the k-fold cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

clf_plants = DecisionTreeClassifier(random_state=42)

# Define grid search
grid_search = RandomizedSearchCV(estimator=clf_plants, 
                           param_distributions=param_grid, 
                           scoring=scoring, 
                           refit='accuracy_score', 
                           n_jobs=-1, 
                           cv=kfold, 
                           verbose=0)
# Fit grid search
grid_result = grid_search.fit(X1_train, y1_train.values.ravel())

#For Deployment:
clf_plants = DecisionTreeClassifier(random_state=42,
                                    max_depth=grid_result.best_params_['max_depth'],
                                    max_features=grid_result.best_params_['max_features'],
                                    criterion=grid_result.best_params_['criterion'],
                                    min_samples_split=grid_result.best_params_['min_samples_split'],
                                    min_impurity_decrease = grid_result.best_params_['min_impurity_decrease'],
                                    min_samples_leaf = grid_result.best_params_['min_samples_leaf'],
                                    max_leaf_nodes = grid_result.best_params_['max_leaf_nodes'])
clf_plants.fit(X1_train, y1_train.values.ravel())
with open('/content/drive/MyDrive/EE769/Decision_tree_plants_model.bin','wb') as f_out1: #specify the file where you wnt to save
  pickle.dump(clf_plants,f_out1) #Save the model to file with pickle

# #save the actual decision trees produced by our model
# plt.figure(figsize=(80,40))
# tree.plot_tree(clf_plants,filled=True)  
# plt.savefig('/content/drive/MyDrive/EE769/tree_plants.eps',format='eps',bbox_inches = "tight")
# print('-------------------------------------------------------------------\n')
# plt.close()

"""## Task-4A: Predictor-3A (Diseases Prediction)
### Decision Tree - Random Search CV - Training
"""

#Decision Tree

#References:
#1. https://towardsdatascience.com/decision-tree-in-python-b433ae57fb93
#2. https://mathtuition88.com/2019/10/11/how-to-save-sklearn-tree-plot-as-file-vector-graphics/

max_depth_range = [18,19,20]
min_samples_split_range = np.logspace(-5, -4, 3, endpoint=True)
min_impurity_decrease_range = np.logspace(-5, -4, 3, endpoint=True)
min_samples_leaf_range = np.logspace(-9, -7, 3, endpoint=True)
max_leaf_nodes_range = [1000,1200,1500]
# min_samples_split_range = np.linspace(0.0001, 0.001, 3, endpoint=True)
print(f'The list of values for max_depth are {max_depth_range}')
print(f'The list of values for min_samples_split are {min_samples_split_range}')
print(f'The list of values for min_impurity_decrease are {min_impurity_decrease_range}')
print(f'The list of values for min_samples_leaf are {min_samples_leaf_range}')
print(f'The list of values for max_leaf_nodes are {max_leaf_nodes_range}')
print('-------------------------------------------------------------------\n')
#Hyperparameter Tuning Using Grid Search
param_grid = { 
    "max_depth": max_depth_range,
    "max_features" : ['sqrt','log2',None],
    "criterion" : ['gini','entropy','log_loss'],
    "min_samples_split" : min_samples_split_range,
    "min_impurity_decrease" : min_impurity_decrease_range,
    "min_samples_leaf" : min_samples_leaf_range,
    "max_leaf_nodes" : max_leaf_nodes_range
    }

# Set up score
scoring = {
    'accuracy_score': make_scorer(accuracy_score),
    'balanced_accuracy_score': make_scorer(balanced_accuracy_score),
    # 'recall_score': make_scorer(recall_score)
    # 'precision_score': make_scorer(precision_score)
    # 'roc_auc_score': make_scorer(roc_auc_score)
    'cohen_kappa_score': make_scorer(cohen_kappa_score)
}
# Set up the k-fold cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

clf_diseases = DecisionTreeClassifier(random_state=42)

# Define grid search
grid_search = RandomizedSearchCV(estimator=clf_diseases, 
                           param_distributions=param_grid, 
                           scoring=scoring, 
                           refit='accuracy_score', 
                           n_jobs=-1, 
                           cv=kfold, 
                           verbose=0)
# Fit grid search
grid_result = grid_search.fit(X2_train, y2_train.values.ravel())

#For Deployment:
clf_diseases = DecisionTreeClassifier(random_state=42,
                                    max_depth=grid_result.best_params_['max_depth'],
                                    max_features=grid_result.best_params_['max_features'],
                                    criterion=grid_result.best_params_['criterion'],
                                    min_samples_split=grid_result.best_params_['min_samples_split'],
                                    min_impurity_decrease = grid_result.best_params_['min_impurity_decrease'],
                                    min_samples_leaf = grid_result.best_params_['min_samples_leaf'],
                                    max_leaf_nodes = grid_result.best_params_['max_leaf_nodes'])
clf_diseases.fit(X2_train, y2_train.values.ravel())
with open('/content/drive/MyDrive/EE769/Decision_tree_diseases_model.bin','wb') as f_out2: #specify the file where you wnt to save
  pickle.dump(clf_diseases,f_out2) #Save the model to file with pickle

# #save the actual decision trees produced by our model
# plt.figure(figsize=(80,40))
# tree.plot_tree(clf_diseases,filled=True)  
# plt.savefig('/content/drive/MyDrive/EE769/tree_diseases.eps',format='eps',bbox_inches = "tight")
# plt.close()

"""## Task-4B Predictor-3A
### Validation - Decision Tree
"""

#function to plot AUC ROC Curve for multi Classification
def plot_auc_roc_multi(y_valid, y_classes, pred_prob2,model_label):
  colors = ['orange','green','blue','red','purple','brown','pink','gray','olive','cyan','salmon','tan','yellow','greenyellow','aqua','beige','lime','dodgerblue','magenta','tomato','teal','silver']
  fpr = {}
  tpr = {}
  thresh ={}
  n_class = len(y_classes)
  for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_valid, pred_prob2[:,i], pos_label=y_classes[i])
    # plotting 
    plt.plot(fpr[i], tpr[i], linestyle='--',color=colors[i], label=y_classes[i] + ' vs Rest')

  plt.title(model_label)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  plt.show()
  return
#####END of function#############

with open('/content/drive/MyDrive/EE769/Decision_tree_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Decision_tree_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)


plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_valid_predicted = clf_plants_loaded.predict(X1_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_valid, y1_valid_predicted), columns=y1_valid_classes, index=y1_valid_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Decision_tree_Plants.png')

print('Plants Classifaction:')
print(f'\tThe hyperparameters are {clf_plants_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y1_valid, y1_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y1_valid, y1_valid_predicted):.4f}')
pred_prob1 = clf_plants_loaded.predict_proba(X1_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y1_valid, pred_prob1,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y1_valid, y1_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_valid_predicted = clf_diseases_loaded.predict(X2_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_valid, y2_valid_predicted), columns=y2_classes, index=y2_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Decision_tree_Diseases.png')

print('Diseases Classifaction:')
print(f'\tThe hyperparameters are {clf_diseases_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y2_valid, y2_valid_predicted):.4f}')
pred_prob2 = clf_diseases_loaded.predict_proba(X2_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y2_valid, pred_prob2,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y2_valid, y2_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')


plot_auc_roc_multi(y1_valid, y1_classes, pred_prob1,'Plants')
print('-------------------------------------------------------------------\n')
plot_auc_roc_multi(y2_valid, y2_classes, pred_prob2,'Diseases')
print('-------------------------------------------------------------------\n')

"""## Task-4C Predictor-3A
###Deployment trail run - Decision Tree

1. https://freecontent.manning.com/deploying-machine-learning-models-part-1-saving-models/
"""

# import pandas as pd
# import numpy as np
# from sklearn.ensemble import RandomForestClassifier
# import pickle

# import torchvision.models as models
# import torchvision.transforms as transforms
# import torch
# from torch.autograd import Variable
# from PIL import Image
# from torchvision.models import ResNet18_Weights
# import warnings
# import time
# import os
# import copy
# from google.colab import drive
# drive.mount('/content/drive')

with open('/content/drive/MyDrive/EE769/Decision_tree_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Decision_tree_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

# print(clf_plants_loaded.get_params())
# print(clf_diseases_loaded.get_params())

#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')
#Set model to evaluation mode
model.eval()
#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()
def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)
  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)
  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)
  #Run the model on our transformed image
  model(t_img)
  # Detach our copy function from the layer
  h.remove()
  # Return the feature vector
  return features_of_image

features_list = []
for x in range(512):
  features_list.append(str(x))

#input image
input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Apple/Apple_scab/AppleScab1.JPG'
input_data_array = pd.DataFrame([extract_ResNet18_features(input_image).numpy()],columns=features_list)
display(input_data_array)
print('The Predicted plant is '+ str(clf_plants_loaded.predict(input_data_array)[0]) )
print('The Predicted disease is '+ str(clf_diseases_loaded.predict(input_data_array)[0]) )

"""## Task-5A: Predictor-3B (Plants Prediction)
### Adaboost - Decision Tree - Random Search CV - Training
"""

#References:
#1.https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html

#Adaboost-Decision tree
with open('/content/drive/MyDrive/EE769/Decision_tree_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

print(clf_plants_loaded.get_params())

n_estimators_range = [50,75,100,150,200]
learning_rate_range = [0.1,0.2,0.4,0.8,1.0]

print(f'The list of values for n_estimators are {n_estimators_range}')
print(f'The list of values for learning_rate are {learning_rate_range}')
print('-------------------------------------------------------------------\n')
#Hyperparameter Tuning Using Grid Search
param_grid = { 
    "n_estimators": n_estimators_range,
    "learning_rate" : learning_rate_range
    }

# Set up score
scoring = {
    'accuracy_score': make_scorer(accuracy_score),
    'balanced_accuracy_score': make_scorer(balanced_accuracy_score),
    # 'recall_score': make_scorer(recall_score)
    # 'precision_score': make_scorer(precision_score)
    # 'roc_auc_score': make_scorer(roc_auc_score)
    'cohen_kappa_score': make_scorer(cohen_kappa_score)
}
# Set up the k-fold cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

clf_plants = AdaBoostClassifier(random_state=42,estimator=clf_plants_loaded)

# Define grid search
grid_search = RandomizedSearchCV(estimator=clf_plants, 
                           param_distributions=param_grid, 
                           scoring=scoring, 
                           refit='accuracy_score', 
                           n_jobs=-1, 
                           cv=kfold, 
                           verbose=0)
# Fit grid search
grid_result = grid_search.fit(X1_train, y1_train.values.ravel())

# For Deployment:
clf_plants = DecisionTreeClassifier(random_state=42,
                                    estimator=clf_plants_loaded,
                                    n_estimators=grid_result.best_params_['n_estimators'],
                                    learning_rate=grid_result.best_params_['learning_rate'])
clf_plants.fit(X1_train, y1_train.values.ravel())
with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_plants_model.bin','wb') as f_out1: #specify the file where you wnt to save
  pickle.dump(clf_plants,f_out1) #Save the model to file with pickle

"""## Task-5A: Predictor-3B (Diseases Prediction)
### Adaboost - Decision Tree - Training
"""

#Adaboost-Decision tree
with open('/content/drive/MyDrive/EE769/Decision_tree_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

print(clf_diseases_loaded.get_params())

# n_estimators_range = [50]
# learning_rate_range = [1.0]

# print(f'The list of values for n_estimators are {n_estimators_range}')
# print(f'The list of values for learning_rate are {learning_rate_range}')
# print('-------------------------------------------------------------------\n')
#Hyperparameter Tuning Using Grid Search
# param_grid = { 
#     "n_estimators": n_estimators_range,
#     "learning_rate" : learning_rate_range
#     }

# Set up score
# scoring = {
#     'accuracy_score': make_scorer(accuracy_score),
#     'balanced_accuracy_score': make_scorer(balanced_accuracy_score),
#     # 'recall_score': make_scorer(recall_score)
#     # 'precision_score': make_scorer(precision_score)
#     # 'roc_auc_score': make_scorer(roc_auc_score)
#     'cohen_kappa_score': make_scorer(cohen_kappa_score)
# }
# Set up the k-fold cross-validation
# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

clf_diseases = AdaBoostClassifier(random_state=42,estimator=clf_diseases_loaded)
# Define grid search
# grid_search = RandomizedSearchCV(estimator=clf_diseases, 
#                            param_distributions=param_grid, 
#                            scoring=scoring, 
#                            refit='accuracy_score', 
#                            n_jobs=-1, 
#                            cv=kfold, 
#                            verbose=True)
# Fit grid search
# grid_result = grid_search.fit(X2_train, y2_train.values.ravel())
clf_diseases.fit(X2_train, y2_train.values.ravel())

# For Deployment:
# clf_diseases = DecisionTreeClassifier(random_state=42,
#                                     estimator=clf_diseases_loaded,
#                                     n_estimators=grid_result.best_params_['n_estimators'],
#                                     learning_rate=grid_result.best_params_['learning_rate'])
# clf_diseases.fit(X2_train, y2_train.values.ravel())
with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_diseases_model.bin','wb') as f_out2: #specify the file where you wnt to save
  pickle.dump(clf_diseases,f_out2) #Save the model to file with pickle

"""## Task-5B: Predictor-3B
###Validation - Adaboost Decision Tree
"""

#function to plot AUC ROC Curve for multi Classification
def plot_auc_roc_multi(y_valid, y_classes, pred_prob2,model_label):
  colors = ['orange','green','blue','red','purple','brown','pink','gray','olive','cyan','salmon','tan','yellow','greenyellow','aqua','beige','lime','dodgerblue','magenta','tomato','teal','silver']
  fpr = {}
  tpr = {}
  thresh ={}
  n_class = len(y_classes)
  for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_valid, pred_prob2[:,i], pos_label=y_classes[i])
    # plotting 
    plt.plot(fpr[i], tpr[i], linestyle='--',color=colors[i], label=y_classes[i] + ' vs Rest')

  plt.title(model_label)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  plt.show()
  return
#####END of function#############

with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_valid_predicted = clf_plants_loaded.predict(X1_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_valid, y1_valid_predicted), columns=y1_valid_classes, index=y1_valid_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_Plants.png')

print('Plants Classifaction:')
print(f'\tThe hyperparameters are {clf_plants_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y1_valid, y1_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y1_valid, y1_valid_predicted):.4f}')
pred_prob1 = clf_plants_loaded.predict_proba(X1_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y1_valid, pred_prob1,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y1_valid, y1_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_valid_predicted = clf_diseases_loaded.predict(X2_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_valid, y2_valid_predicted), columns=y2_classes, index=y2_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_Diseases.png')

print('Diseases Classifaction:')
# print(f'\tThe best hyperparameters are {grid_result.best_params_}')
print(f'\tThe hyperparameters are {clf_diseases_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y2_valid, y2_valid_predicted):.4f}')
pred_prob2 = clf_diseases_loaded.predict_proba(X2_valid)
# print(f'\tThe roc_auc score for the validation dataset is {roc_auc_score(y2_valid, pred_prob2,multi_class="ovr"):.4f}')
print('\tClassification Report:')
print(classification_report(y2_valid, y2_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

plot_auc_roc_multi(y1_valid, y1_classes, pred_prob1,'Plants')
plot_auc_roc_multi(y2_valid, y2_classes, pred_prob2,'Diseases')

"""## Task-5C: Predictor-3B
###Deployment trail run - Adaboost Decision Tree

1. https://freecontent.manning.com/deploying-machine-learning-models-part-1-saving-models/
"""

with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

# print(clf_plants_loaded.get_params())
# print(clf_diseases_loaded.get_params())

#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')
#Set model to evaluation mode
model.eval()
#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()
def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)
  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)
  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)
  #Run the model on our transformed image
  model(t_img)
  # Detach our copy function from the layer
  h.remove()
  # Return the feature vector
  return features_of_image

features_list = []
for x in range(512):
  features_list.append(str(x))

#input image
input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Apple/Apple_scab/AppleScab1.JPG'
input_data_array = pd.DataFrame([extract_ResNet18_features(input_image).numpy()],columns=features_list)
display(input_data_array)
print('The Predicted plant is '+ str(clf_plants_loaded.predict(input_data_array)[0]) )
print('The Predicted disease is '+ str(clf_diseases_loaded.predict(input_data_array)[0]) )

"""## Task-6A: Predictor-4
### Voting Classifier (Ensemble Learning Model) - Plants Classification - Training
"""

#Voting Classifier

#References:
#1. https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a


with open('/content/drive/MyDrive/EE769/L2_regularized_plants_model.bin','rb') as f_in1:
  l2_plants = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Random_forest_plants_model_Random.bin','rb') as f_in2:
  rf_plants = pickle.load(f_in2)

with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_plants_model.bin','rb') as f_in3:
  dt_plants = pickle.load(f_in3)

#create a dictionary of our models
estimators=[('l2', l2_plants), ('rf', rf_plants), ('dt', dt_plants)]

#create our voting classifier, inputting our models
ensemble_plants = VotingClassifier(estimators, voting='hard')
ensemble_plants.fit(X1_train, y1_train.values.ravel())



#For Deployment:
with open('/content/drive/MyDrive/EE769/Voting_classifier_plants_model.bin','wb') as f_out1: #specify the file where you wnt to save
  pickle.dump(ensemble_plants,f_out1) #Save the model to file with pickle

"""## Task-6A: Predictor-4
### Voting Classifier (Ensemble Learning Model) - Diseases Classification - Training
"""

#Voting Classifier

#References:
#1. https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a

with open('/content/drive/MyDrive/EE769/L2_regularized_diseases_model.bin','rb') as f_in4:
  l2_diseases = pickle.load(f_in4)

with open('/content/drive/MyDrive/EE769/Random_forest_diseases_model_Random.bin','rb') as f_in5:
  rf_diseases = pickle.load(f_in5)

with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_diseases_model.bin','rb') as f_in6:
  dt_diseases = pickle.load(f_in6)

#create a dictionary of our models
estimators=[('l2', l2_diseases), ('rf', rf_diseases), ('dt', dt_diseases)]

#create our voting classifier, inputting our models
ensemble_diseases = VotingClassifier(estimators, voting='hard')
ensemble_diseases.fit(X2_train, y2_train.values.ravel())

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_valid_predicted = ensemble_diseases.predict(X2_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_valid, y2_valid_predicted), columns=y2_classes, index=y2_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Voting_classifier_Diseases.png')
print('Diseases Classifaction:')

print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print('\tClassification Report:')
print(classification_report(y2_valid, y2_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

#For Deployment:
with open('/content/drive/MyDrive/EE769/Voting_classifier_diseases_model.bin','wb') as f_out2: #specify the file where you wnt to save
  pickle.dump(ensemble_diseases,f_out2) #Save the model to file with pickle

"""## Task-6B: Predictor-4
###Validation - Voting classifier
"""

with open('/content/drive/MyDrive/EE769/Voting_classifier_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Voting_classifier_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

#Plants
plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_valid_predicted = clf_plants_loaded.predict(X1_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_valid, y1_valid_predicted), columns=y1_valid_classes, index=y1_valid_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Voting_classifier_Plants.png')

print('Plants Classifaction:')
print(f'\tThe hyperparameters are {clf_plants_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y1_valid, y1_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y1_valid, y1_valid_predicted):.4f}')
print('\tClassification Report:')
print(classification_report(y1_valid, y1_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

#Diseases
plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_valid_predicted = clf_diseases_loaded.predict(X2_valid)
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_valid, y2_valid_predicted), columns=y2_classes, index=y2_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Voting_classifier_Diseases.png')
print('Diseases Classifaction:')
print(f'\tThe hyperparameters are {clf_diseases_loaded.get_params()}')
print(f'\tThe accuracy score for the validation dataset is {accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print(f'\tThe balanced accuracy score for the validation dataset is {balanced_accuracy_score(y2_valid, y2_valid_predicted):.4f}')
print('\tClassification Report:')
print(classification_report(y2_valid, y2_valid_predicted))
plt.show()
print('-------------------------------------------------------------------\n')

"""## Task-6C: Predictor-4
###Deployment trail run - Voting classifier

1. https://freecontent.manning.com/deploying-machine-learning-models-part-1-saving-models/
"""

with open('/content/drive/MyDrive/EE769/Voting_classifier_plants_model.bin','rb') as f_in1:
  clf_plants_loaded = pickle.load(f_in1)

with open('/content/drive/MyDrive/EE769/Voting_classifier_diseases_model.bin','rb') as f_in2:
  clf_diseases_loaded = pickle.load(f_in2)

# print(clf_plants_loaded.get_params())
# print(clf_diseases_loaded.get_params())

#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')
#Set model to evaluation mode
model.eval()
#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()
def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)
  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)
  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)
  #Run the model on our transformed image
  model(t_img)
  # Detach our copy function from the layer
  h.remove()
  # Return the feature vector
  return features_of_image

features_list = []
for x in range(512):
  features_list.append(str(x))

#input image
input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Apple/Apple_scab/AppleScab1.JPG'
input_data_array = pd.DataFrame([extract_ResNet18_features(input_image).numpy()],columns=features_list)
display(input_data_array)
print('The Predicted plant is '+ str(clf_plants_loaded.predict(input_data_array)[0]) )
print('The Predicted disease is '+ str(clf_diseases_loaded.predict(input_data_array)[0]) )

"""## Task-7
### Testing 
* Test the final model on test data and show the results -- accuracy and F1 score.
"""

#Test Data
warnings.filterwarnings('ignore')
#References:
#1. https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c
#2. https://stackoverflow.com/questions/61606416/runtimeerror-output-with-shape-512-doesnt-match-the-broadcast-shape-1-512


#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')

#Set model to evaluation mode
model.eval()

#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()

def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)

  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))

  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)

  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)

  #Run the model on our transformed image
  model(t_img)

  # Detach our copy function from the layer
  h.remove()

  # Return the feature vector
  return features_of_image.numpy()


path =  '/content/drive/MyDrive/EE769/project_dataset/test'

plants_list = []  #['Orange', 'Peach', 'Squash', 'Strawberry', 'Soyabean', 'Corn_maize', 'Apple', 'Grape', 'Potato', 'Pepper', 'Raspberry', 'Blueberry', 'Tomato', 'Cherry']
diseases_dict = {}
obj = os.scandir(path)
for entry in obj:
    if entry.is_dir():
        plants_list.append(entry.name)

print(plants_list)

features_array = []
target_plants_array = []
target_diseases_array = []

for x in plants_list:
  plants_path = path + str('/') + x
  temp_list = []
  obj = os.scandir(plants_path)
  for entry in obj:
    if entry.is_dir():
      temp_list.append(entry.name)
  diseases_dict[x] = temp_list
  for y in temp_list:
    disease_path = plants_path+ str('/') + y
    obj = os.scandir(disease_path)
    # count = 0
    for entry in obj:
      # if count<50:
      if entry.is_file():
         file_path = disease_path + str('/') + entry.name
         features_array.append(extract_ResNet18_features(file_path))
         target_plants_array.append(x)
         target_diseases_array.append(y)
      #   count += 1
      # else:
      #   break

rows = len(features_array)
cols = len(features_array[0])

print('-------------------------------------------------------------------\n')
print('Dimensions of features array list is ' + str(rows) + ' X ' + str(cols) + '\n')
print('-------------------------------------------------------------------\n')

#Converting feature data, target data into dataframes
features_list = []
for x in range(cols):
  features_list.append(str(x))
X = pd.DataFrame(features_array, columns = features_list)
y1_test = pd.DataFrame({'plant':target_plants_array})
y2_test = pd.DataFrame({'disease':target_diseases_array})

y1_test_classes = np.unique(y1_test) #plants
y2_test_classes = np.unique(y2_test) #diseases

print(y1_test.value_counts().reset_index())
print(y2_test.value_counts().reset_index())

# # Multivariate feature imputation
# imp = IterativeImputer(max_iter=10, random_state=10)
# imp = imp.fit(X)
# X1_train = imp.transform(X)
# X1_train = pd.DataFrame(X1_train, columns = X.columns)

X1_test = X
X2_test = X1_test


# df = pd.concat([X1_test, y1_test, y2_test], axis=1)
# df.to_csv('/content/drive/MyDrive/EE769/data_test.csv')

print('The obtained features data array is:')
display(pd.concat([X1_test, y1_test, y2_test], axis=1))
print('-------------------------------------------------------------------\n')
print('-------------------------------------------------------------------\n')

with open('/content/drive/MyDrive/EE769/L2_regularized_plants_model.bin','rb') as f_in1:
  l2_plants = pickle.load(f_in1)
with open('/content/drive/MyDrive/EE769/Random_forest_plants_model_Random.bin','rb') as f_in2:
  rf_plants = pickle.load(f_in2)
with open('/content/drive/MyDrive/EE769/Decision_tree_plants_model.bin','rb') as f_in3A:
  dt_plants = pickle.load(f_in3A)
with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_plants_model.bin','rb') as f_in3B:
  ada_plants = pickle.load(f_in3B)
with open('/content/drive/MyDrive/EE769/Voting_classifier_plants_model.bin','rb') as f_in4:
  vote_plants = pickle.load(f_in4)
with open('/content/drive/MyDrive/EE769/L2_regularized_diseases_model.bin','rb') as f_in5:
  l2_diseases = pickle.load(f_in5)
with open('/content/drive/MyDrive/EE769/Random_forest_diseases_model_Random.bin','rb') as f_in6:
  rf_diseases = pickle.load(f_in6)
with open('/content/drive/MyDrive/EE769/Decision_tree_diseases_model.bin','rb') as f_in7A:
  dt_diseases = pickle.load(f_in7A)
with open('/content/drive/MyDrive/EE769/Adaboost_Decision_tree_diseases_model.bin','rb') as f_in7B:
  ada_diseases = pickle.load(f_in7B)
with open('/content/drive/MyDrive/EE769/Voting_classifier_diseases_model.bin','rb') as f_in8:
  vote_diseases = pickle.load(f_in8)

print('L2 regularized logistic regression')
print('\tPlants Classification')
y1_test_predicted = l2_plants.predict(X1_test)
pred_prob1 = l2_plants.predict_proba(X1_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y1_test, y1_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y1_test, y1_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y1_test, y1_test_predicted))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_test_pred_classes = np.unique(y1_test_predicted) #plants
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_test, y1_test_predicted), columns=y1_test_pred_classes, index=y1_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_L2_Plants.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('\tDiseases Classification')
y2_test_predicted = l2_diseases.predict(X2_test)
pred_prob2 = l2_diseases.predict_proba(X2_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y2_test, y2_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y2_test, y2_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y2_test, y2_test_predicted))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_test_pred_classes = np.unique([*y2_test_classes , *np.unique(y2_test_predicted)]) #diseases
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_test, y2_test_predicted), columns=y2_test_pred_classes, index=y2_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_L2_Diseases.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('-------------------------------------------------------------------\n')
print('Random Forest')
print('\tPlants Classification')
y1_test_predicted = rf_plants.predict(X1_test)
pred_prob1 = rf_plants.predict_proba(X1_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y1_test, y1_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y1_test, y1_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y1_test, y1_test_predicted))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_test_pred_classes = np.unique(y1_test_predicted) #plants
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_test, y1_test_predicted), columns=y1_test_pred_classes, index=y1_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Random_Forest_Plants.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('\tDiseases Classification')
y2_test_predicted = rf_diseases.predict(X2_test)
pred_prob2 = rf_diseases.predict_proba(X2_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y2_test, y2_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y2_test, y2_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y2_test, y2_test_predicted))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_test_pred_classes = np.unique([*y2_test_classes , *np.unique(y2_test_predicted)]) #diseases
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_test, y2_test_predicted), columns=y2_test_pred_classes, index=y2_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Random_Forest_Diseases.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('-------------------------------------------------------------------\n')
print('Decision Tree')
print('\tPlants Classification')
y1_test_predicted = dt_plants.predict(X1_test)
pred_prob1 = dt_plants.predict_proba(X1_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y1_test, y1_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y1_test, y1_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y1_test, y1_test_predicted))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_test_pred_classes = np.unique(y1_test_predicted) #plants
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_test, y1_test_predicted), columns=y1_test_pred_classes, index=y1_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Decision_tree_Plants.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('\tDiseases Classification')
y2_test_predicted = dt_diseases.predict(X2_test)
pred_prob2 = dt_diseases.predict_proba(X2_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y2_test, y2_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y2_test, y2_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y2_test, y2_test_predicted))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_test_pred_classes = np.unique([*y2_test_classes , *np.unique(y2_test_predicted)]) #diseases
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_test, y2_test_predicted), columns=y2_test_pred_classes, index=y2_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Decision_tree_Diseases.png')
plt.show()

print('-------------------------------------------------------------------\n')
print('-------------------------------------------------------------------\n')

print('Adaboost-Decision Tree')
print('\tPlants Classification')
y1_test_predicted = ada_plants.predict(X1_test)
pred_prob1 = ada_plants.predict_proba(X1_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y1_test, y1_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y1_test, y1_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y1_test, y1_test_predicted))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_test_pred_classes = np.unique(y1_test_predicted) #plants
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_test, y1_test_predicted), columns=y1_test_pred_classes, index=y1_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Adaboost_Decision_tree_Plants.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('\tDiseases Classification')
y2_test_predicted = ada_diseases.predict(X2_test)
pred_prob2 = ada_diseases.predict_proba(X2_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y2_test, y2_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y2_test, y2_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y2_test, y2_test_predicted))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_test_pred_classes = np.unique([*y2_test_classes , *np.unique(y2_test_predicted)]) #diseases
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_test, y2_test_predicted), columns=y2_test_pred_classes, index=y2_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Adaboost_Decision_tree_Diseases.png')
plt.show()

print('-------------------------------------------------------------------\n')
print('-------------------------------------------------------------------\n')
print('Voting Classifier')
print('\tPlants Classification')
y1_test_predicted = vote_plants.predict(X1_test)
# pred_prob1 = vote_plants.predict_proba(X1_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y1_test, y1_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y1_test, y1_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y1_test, y1_test_predicted))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
y1_test_pred_classes = np.unique(y1_test_predicted) #plants
confusion_matrix_df = pd.DataFrame(confusion_matrix(y1_test, y1_test_predicted), columns=y1_test_pred_classes, index=y1_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Voting_Classifier_Plants.png')
plt.show()
print('-------------------------------------------------------------------\n')
print('\tDiseases Classification')
y2_test_predicted = vote_diseases.predict(X2_test)
# pred_prob2 = vote_diseases.predict_proba(X2_test)
print(f'\tThe accuracy score for the testing dataset is {accuracy_score(y2_test, y2_test_predicted):.4f}')
print(f'\tThe balanced accuracy score for the testing dataset is {balanced_accuracy_score(y2_test, y2_test_predicted):.4f}')
# print(f'\tThe roc_auc score for the testing dataset is {roc_auc_score(y1_test, pred_prob1[:,1]):.4f}')
print('\tClassification Report:')
print(classification_report(y2_test, y2_test_predicted))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
y2_test_pred_classes = np.unique([*y2_test_classes , *np.unique(y2_test_predicted)]) #diseases
confusion_matrix_df = pd.DataFrame(confusion_matrix(y2_test, y2_test_predicted), columns=y2_test_pred_classes, index=y2_test_pred_classes)
sns.heatmap(confusion_matrix_df, annot=True)
plt.savefig('/content/drive/MyDrive/EE769/Testing_Voting_Classifier_Diseases.png')
plt.show()

"""## Task-8A: Predictor-5
### Set up the GPU
"""

# Set up the GPU device
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""## Task-8A: Predictor-5
### Deep Learning Model - Plants Classification - Training
"""

# Set up the training and validation data
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()

# Define the number of features and classes in your data
num_features = X1_train.shape[1]
num_classes = len(y1_classes)

label_encoder = LabelEncoder()
y1_train = label_encoder.fit_transform(y1_train)
y1_valid = label_encoder.transform(y1_valid)

# Convert the output labels to one-hot encoded form
y1_train = to_categorical(y1_train, num_classes=num_classes)
y1_valid = to_categorical(y1_valid, num_classes=num_classes)

# Define the number of features and classes in your data
print(num_features)
print(num_classes)

# create plant model 
plant_model = Sequential()
plant_model.add(Dense(512, activation='relu', input_shape=(num_features,)))
plant_model.add(Dropout(0.5))
plant_model.add(Dense(256, activation='relu'))
plant_model.add(Dropout(0.5))
plant_model.add(Dense(128, activation='relu'))
plant_model.add(Dropout(0.5))
plant_model.add(Dense(64, activation='relu'))
plant_model.add(Dropout(0.5))
plant_model.add(Dense(32, activation='relu'))
plant_model.add(Dropout(0.5))
plant_model.add(Dense(num_classes, activation='softmax'))

# # compile plant model
plant_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Set the parameters for training the model
num_epochs = 200
batch_size = 32
steps_per_epoch = int(np.ceil(len(X1_train) / batch_size))

# train plant model
with tf.device('/device:GPU:0'):
  plant_model.fit(
      X1_train,
      y1_train,
      epochs=num_epochs,
      steps_per_epoch = steps_per_epoch,
      validation_data=(X1_valid,y1_valid))

# save plant model
plant_model.save('/content/drive/MyDrive/EE769/plant_model.h5')

"""## Task-8A: Predictor-5
### Deep Learning Model - Creating diseases models for each plant - Training
"""

# Set up the training and validation data
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()

# Define the number of features and classes in your data
num_features = X2_train.shape[1]
num_classes = len(y2_classes)

label_encoder = LabelEncoder()
y2_train = label_encoder.fit_transform(y2_train)
y2_valid = label_encoder.transform(y2_valid)

# Convert the output labels to one-hot encoded form
y2_train = to_categorical(y2_train, num_classes=num_classes)
y2_valid = to_categorical(y2_valid, num_classes=num_classes)


# Load the plant model and remove the last layer
plant_model = load_model('/content/drive/MyDrive/EE769/plant_model.h5')
plant_model.layers.pop()

# Freeze the layers in the plant model
for layer in plant_model.layers:
    layer.trainable = False

def get_plant_data_indexes(plant_folder):
  data_train = pd.read_csv('/content/drive/MyDrive/EE769/data_tain.csv')
  data_valid = pd.read_csv('/content/drive/MyDrive/EE769/data_valid.csv')
  train_indexes = data_train.index[data_train['plant'] == plant_folder].tolist()
  valid_indexes = data_valid.index[data_valid['plant'] == plant_folder].tolist()
  return train_indexes,valid_indexes

# create disease models using transfer learning
for plant_folder in y1_classes:
    print(plant_folder)
    train_indexes,valid_indexes = get_plant_data_indexes(plant_folder)
    X2_train_plant = X2_train.iloc[train_indexes]
    y2_train_plant = y2_train[train_indexes]
    X2_valid_plant = X2_valid.iloc[valid_indexes]
    y2_valid_plant = y2_valid[valid_indexes]

    disease_model_name = plant_folder+"_disease_model"
    disease_model = Sequential(name=disease_model_name)
    disease_model.add(plant_model)
    disease_model.add(Flatten())
    disease_model.add(Dense(512, activation='relu'))
    disease_model.add(Dropout(0.5))
    disease_model.add(Dense(256, activation='relu'))
    disease_model.add(Dropout(0.5))
    disease_model.add(Dense(128, activation='relu'))
    disease_model.add(Dropout(0.5))
    disease_model.add(Dense(64, activation='relu'))
    disease_model.add(Dropout(0.5))
    disease_model.add(Dense(32, activation='relu'))
    disease_model.add(Dropout(0.5))
    disease_model.add(Dense(num_classes, activation='softmax'))
    disease_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 

    # Set the parameters for training the model
    num_epochs = 25
    batch_size = 16
    steps_per_epoch = int(np.ceil(len(X2_train_plant) / batch_size))

    with tf.device('/device:GPU:0'):
      disease_model.fit(
      X2_train_plant,
      y2_train_plant,
      epochs=num_epochs,
      steps_per_epoch = steps_per_epoch,
      validation_data=(X2_valid_plant,y2_valid_plant))
    disease_model.save(f'/content/drive/MyDrive/EE769/models/{plant_folder}_disease_model.h5')

"""## Task-8A: Predictor-5
### Deep Learning Model - Merging all the disease models into a single model - Training
"""

# Set up the training and validation data
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()

# Set the parameters for training the model
num_epochs = 50
batch_size = 32
steps_per_epoch = int(np.ceil(len(X2_train) / batch_size))

# Define the number of features and classes in your data
num_features = X2_train.shape[1]
num_classes = len(y2_classes)

label_encoder = LabelEncoder()
y2_train = label_encoder.fit_transform(y2_train)
y2_valid = label_encoder.transform(y2_valid)

# Convert the output labels to one-hot encoded form
y2_train = to_categorical(y2_train, num_classes=num_classes)
y2_valid = to_categorical(y2_valid, num_classes=num_classes)


#merging diseases models 
disease_models_path = '/content/drive/MyDrive/EE769/models'

disease_models = []

for filename in os.listdir(disease_models_path):
    if filename.endswith('.h5'):
        model_path = os.path.join(disease_models_path, filename)
        model = load_model(model_path)
        # Create a new input layer with the desired name
        new_input = Input(shape=model.input_shape[1:], name=filename)
        hidden_layer = Dense(64, activation='relu')(new_input)
        output_layer = Dense(num_classes, activation='softmax')(hidden_layer)
        # Create a new model with the new input layer
        new_model = Model(inputs=new_input, outputs=output_layer)
        new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        disease_models.append(new_model)

# merge models
input_layer = Input(shape=(num_features,))
outputs = [model(input_layer) for model in disease_models]
concatenated_output = Concatenate()(outputs)
hidden_layer = Dense(128, activation='relu')(concatenated_output) # hidden layer
final_output = Dense(num_classes, activation='softmax')(hidden_layer) #num_classes =21 (num of unique diseases)
merged_disease_model = Model(inputs=input_layer, outputs=final_output)

# compile new model
merged_disease_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# merged_disease_model.summary()
with tf.device('/device:GPU:0'):
  merged_disease_model.fit(
      X2_train,
      y2_train,
      epochs=num_epochs,
      steps_per_epoch = steps_per_epoch,
      validation_data=(X2_valid_plant,y2_valid_plant))
  merged_disease_model.save(f'/content/drive/MyDrive/EE769/merged_disease_model.h5')

merged_disease_model.summary()

"""## Task-8B: Predictor-5
### Deep Learning Model - Validation
"""

# Set up the training and validation data
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()

label_encoder = LabelEncoder()
y1_train = label_encoder.fit_transform(y1_train)
y1_valid = label_encoder.transform(y1_valid)
plant_class_names = label_encoder.classes_

# Convert the output labels to one-hot encoded form
y1_valid = to_categorical(y1_valid, num_classes=len(y1_classes))

#Load plants model
plant_model = load_model('/content/drive/MyDrive/EE769/plant_model.h5')

# assume y1_valid is the one-hot encoded validation labels
plant_pred = plant_model.predict(X1_valid)
plant_pred_labels = np.argmax(plant_pred, axis=1)
plant_true_labels = np.argmax(y1_valid, axis=1)

# print plnats classification report
print('Plants Classifaction:')
print(classification_report(plant_true_labels, plant_pred_labels))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
plants_cm = confusion_matrix(plant_true_labels, plant_pred_labels)
sns.heatmap(plants_cm, annot=True, xticklabels=plant_class_names, yticklabels=plant_class_names)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.savefig('/content/drive/MyDrive/EE769/Plants_model_deep_learning.png')
plt.show()

label_encoder = LabelEncoder()
y2_train = label_encoder.fit_transform(y2_train)
y2_valid = label_encoder.transform(y2_valid)
disease_class_names = label_encoder.classes_

# Convert the output labels to one-hot encoded form
y2_valid = to_categorical(y2_valid, num_classes=len(y2_classes))

#Load disease model
merged_disease_model = load_model('/content/drive/MyDrive/EE769/merged_disease_model.h5')

# assume y2_valid is the one-hot encoded validation labels
disease_pred = merged_disease_model.predict(X2_valid)
disease_pred_labels = np.argmax(disease_pred, axis=1)
disease_true_labels = np.argmax(y2_valid, axis=1)

# print plnats classification report
print('Diseases Classifaction:')
print(classification_report(disease_true_labels, disease_pred_labels))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
disease_cm = confusion_matrix(disease_true_labels, disease_pred_labels)
sns.heatmap(disease_cm, annot=True, xticklabels=disease_class_names, yticklabels=disease_class_names)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.savefig('/content/drive/MyDrive/EE769/Disease_model_deep_learning.png')
plt.show()

"""## Task-8C: Predictor-5
### Deep Learning Model - Testing
"""

# Set up the training and validation data
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()


# Set up the testing data
data_test = pd.read_csv('/content/drive/MyDrive/EE769/data_test.csv')
features_list = list( data_test.loc[:][:1] )[1:-2]
target_list  = list( data_test.loc[:][:1] )[-2:]
#Extracting feature data from csv
X1_test  = data_test.loc[:,features_list]
X2_test = X1_test
#Extracting plants target data from csv
y1_test = data_test.loc[:,target_list[0]]
#Extracting diseases target data from csv
y2_test = data_test.loc[:,target_list[1]]
y1_test_classes = np.unique(y1_test) #plants
y2_test_classes = np.unique(y2_test) #diseases
# ------------------------------------------

label_encoder = LabelEncoder()
y1_train = label_encoder.fit_transform(y1_train)
y1_test = label_encoder.transform(y1_test)
# plant_class_names = label_encoder.classes_

# Convert the output labels to one-hot encoded form
y1_test = to_categorical(y1_test,num_classes=len(y1_classes))

#Load plants model
plant_model = load_model('/content/drive/MyDrive/EE769/plant_model.h5')

# assume y1_test is the one-hot encoded validation labels
plant_pred = plant_model.predict(X1_test)
plant_pred_labels = np.argmax(plant_pred, axis=1)
plant_true_labels = np.argmax(y1_test, axis=1)

labels = np.unique(np.concatenate((plant_true_labels, plant_pred_labels), axis=0))
class_names = label_encoder.inverse_transform(labels)

# print plants classification report
print('Plants Classifaction:')
print(classification_report(plant_true_labels, plant_pred_labels, target_names=class_names))

plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
plants_cm = confusion_matrix(plant_true_labels, plant_pred_labels, labels=labels)
sns.heatmap(plants_cm, annot=True, xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.savefig('/content/drive/MyDrive/EE769/Testing_Plants_model_deep_learning.png')
plt.show()


label_encoder = LabelEncoder()
y2_train = label_encoder.fit_transform(y2_train)
y2_test = label_encoder.transform(y2_test)
# disease_class_names = label_encoder.classes_

# Convert the output labels to one-hot encoded form
y2_test = to_categorical(y2_test, num_classes=len(y2_classes))

#Load disease model
merged_disease_model = load_model('/content/drive/MyDrive/EE769/merged_disease_model.h5')

# assume y2_valid is the one-hot encoded validation labels
disease_pred = merged_disease_model.predict(X2_test)
disease_pred_labels = np.argmax(disease_pred, axis=1)
disease_true_labels = np.argmax(y2_test, axis=1)

labels = np.unique(np.concatenate((disease_true_labels, disease_pred_labels), axis=0))
class_names = label_encoder.inverse_transform(labels)

# print plnats classification report
print('Diseases Classifaction:')
print(classification_report(disease_true_labels, disease_pred_labels))

plt.rcParams["figure.figsize"] = [40.00, 12]
plt.rcParams["figure.autolayout"] = True
disease_cm = confusion_matrix(disease_true_labels, disease_pred_labels,labels=labels)
sns.heatmap(disease_cm, annot=True, xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.savefig('/content/drive/MyDrive/EE769/Testing_Disease_model_deep_learning.png')
plt.show()

"""## Task-8D: Predictor-5
### Deep Learning Model - Saving label encoding as a file
"""

# Set up the training and validation data
X1_train,y1_train,X2_train,y2_train,X1_valid,y1_valid,X2_valid,y2_valid,y1_classes,y2_classes,y1_valid_classes,y2_valid_classes = initialize_train_valid_data()

label_encoder = LabelEncoder()
y1_train = label_encoder.fit_transform(y1_train)
with open('/content/drive/MyDrive/EE769/plants_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

label_encoder = LabelEncoder()
y2_train = label_encoder.fit_transform(y2_train)
with open('/content/drive/MyDrive/EE769/diseases_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

"""## Task-8D: Predictor-5
### Deep Learning Model - Deployment Trail run
"""

#Load models
plant_model = load_model('/content/drive/MyDrive/EE769/plant_model.h5')
disease_model = load_model('/content/drive/MyDrive/EE769/merged_disease_model.h5')

with open('/content/drive/MyDrive/EE769/plants_encoder.pkl', 'rb') as f1:
    plant_encoder = pickle.load(f1)
with open('/content/drive/MyDrive/EE769/diseases_encoder.pkl', 'rb') as f2:
    diseases_encoder = pickle.load(f2)

#Load the pretrained model
# model = models.resnet18(pretrained=True) #The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
model = models.resnet18(weights=ResNet18_Weights.DEFAULT)
#Use the model object to select the desired layer
layer = model._modules.get('avgpool')
#Set model to evaluation mode
model.eval()
#ResNet-18 expects images to be at least 224x224, as well as normalized with a specific mean and standard deviation
scaler = transforms.Resize((224, 224))
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                  std=[0.229, 0.224, 0.225])
to_tensor = transforms.ToTensor()
def extract_ResNet18_features(input_image_path):
  #Load the image with Pillow library
  img = Image.open(input_image_path)
  #Create a PyTorch Variable with the transformed image
  t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
  #Create a vector of zeros that will hold our feature vector
    #The 'avgpool' layer has an output size of 512
  features_of_image = torch.zeros(512)
  #Define a function that will copy the output of a layer
  def copy_data(m, i, o):
    features_of_image.copy_(o.data.reshape(o.data.size(1)))
    # features_of_image.copy_(o.data)
  #Attach that function to our selected layer
  h = layer.register_forward_hook(copy_data)
  #Run the model on our transformed image
  model(t_img)
  # Detach our copy function from the layer
  h.remove()
  # Return the feature vector
  return features_of_image

features_list = []
for x in range(512):
  features_list.append(str(x))

#input image
# input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Apple/Apple_scab/AppleScab1.JPG'
input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Corn_maize/Common_rust/CornCommonRust1.JPG'
# input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Potato/Early_blight/PotatoEarlyBlight1.JPG'
# input_image = '/content/drive/MyDrive/EE769/project_dataset/test/Tomato/Tomato_Yellow_Leaf_Curl_Virus/TomatoYellowCurlVirus2.JPG'

input_data_array = pd.DataFrame([extract_ResNet18_features(input_image).numpy()],columns=features_list)

display(input_data_array)

plants_predictions = plant_model.predict(input_data_array)
disease_predictions = disease_model.predict(input_data_array)

print('The Predicted plant is '+ str(plant_encoder.inverse_transform(np.argmax(plants_predictions, axis=1))[0]) )
print('The Predicted disease is '+ str(diseases_encoder.inverse_transform(np.argmax(disease_predictions, axis=1))[0]) )

"""## Task-8E: Predictor-5
### Deep Learning Model - Plot model architecture
"""

#Load models
plant_model = load_model('/content/drive/MyDrive/EE769/plant_model.h5')
disease_model = load_model('/content/drive/MyDrive/EE769/merged_disease_model.h5')

# Plot the model architecture
plot_model(plant_model, to_file='/content/drive/MyDrive/EE769/architecture_plant_model.png', show_shapes=True, show_layer_names=True)
plot_model(disease_model, to_file='/content/drive/MyDrive/EE769/architecture_merged_disease_model.png', show_shapes=True, show_layer_names=True)

print("Plants model Architecture")
plt.rcParams["figure.figsize"] = [5.00, 7]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots()
axs.imshow(plt.imread('/content/drive/MyDrive/EE769/architecture_plant_model.png'))
axs.axis('off')
plt.show()

print("Disease model Architecture")
plt.rcParams["figure.figsize"] = [20.00, 3.50]
plt.rcParams["figure.autolayout"] = True
fig, axs = plt.subplots()
axs.imshow(plt.imread('/content/drive/MyDrive/EE769/architecture_merged_disease_model.png'))
axs.axis('off')
plt.show()

"""##Task-9: Part-1
###Findings
1. Validation Accuracy:
    * L2 Logistic Regression: Plants - 98.54%, Diseases - 96.39%
    * Random Forest: Plants - 94.91%, Diseases - 88.08%
    * Decision Tree: Plants - 75.75%, Diseases - 68.07%
    * Adaboost Decision Tree: Plants - 90.24%, Diseases - 81.67%
    * Voting classifier: Plants - 95.97%, Diseases - 90.39%
    * Deep Learning Model: Plants - 79%, Diseases - 97%
2. Testing Accuracy:
    * L2 Logistic Regression: Plants - 93.94%, Diseases - 90.91%
    * Random Forest: Plants - 90.91%, Diseases - 66.67%
    * Decision Tree: Plants - 72.73%, Diseases - 54.55%
    * Adaboost Decision Tree: Plants - 96.97%, Diseases - 75.76%
    * Voting classifier: Plants - 96.97%, Diseases - 78.79%
    * Deep Learning Model: Plants - 82%, Diseases - 97%

* Tried to boost the accuracy of Random Forest diseases Classification with Adaboost but there is no much improvement but the binary file doubled in size (450MB to 900MB). 
  * Before Adaboost: Vaildation-88.08%, Testing-66.67%
  * With Adaboost: Vailidation-88.17%, Testing-72.73%
* While Adaboost can be effective in improving the accuracy of some machine learning models, it may not always lead to significant improvements in every situation. The lack of improvement when using Adaboost with Random Forest could be due to a number of factors, such as:
  * The dataset may not have enough features or may not be diverse enough to benefit from boosting.
  * The Random Forest model may already be well-optimized and adding Adaboost may not significantly improve its performance.
  * The hyperparameters of the Adaboost algorithm, such as the number of iterations and learning rate, may not have been tuned properly.

##Task-9: Part-2
###References
1. https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c
2. https://stackoverflow.com/questions/61606416/runtimeerror-output-with-shape-512-doesnt-match-the-broadcast-shape-1-512
3. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
4. https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a
5. https://freecontent.manning.com/deploying-machine-learning-models-part-1-saving-models/
6. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
7. https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/
8. https://towardsdatascience.com/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399
9. https://towardsdatascience.com/decision-tree-in-python-b433ae57fb93
10. https://mathtuition88.com/2019/10/11/how-to-save-sklearn-tree-plot-as-file-vector-graphics/
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
11. https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a
"""